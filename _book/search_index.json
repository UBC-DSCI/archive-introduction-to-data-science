[
["index.html", "Intro to Data Science Chapter 1 Introductory Data Science", " Intro to Data Science Tiffany Timbers Matías Salibián-Barrera Bruce Dunham Melissa Lee Samuel Hinshaw 2018-05-22 Chapter 1 Introductory Data Science This is an open source textbook aimed at introducing undergraduate students to Data Science. "],
["chapter2.html", "Chapter 2 Introduction to Data Science 2.1 Overview 2.2 Learning Objectives", " Chapter 2 Introduction to Data Science 2.1 Overview Learn to use the R programming language and Jupyter notebooks as you walk through a real world Data Science application that includes downloading data from the web, wrangling the data into a useable format and creating an effective data visualization. 2.2 Learning Objectives By the end of the chapter, students will be able to: use a Jupyter notebook to execute provided R code edit code and markdown cells in a Jupyter notebook create new code and markdown cells in a Jupyter notebook load the tidyverse library into R create new variables and objects in R using the assignment symbol use the help and documentation tools in R match the names of the following functions from the tidyverse library to their docmentation descriptions: read_csv select mutate filter ggplot aes "],
["chapter3.html", "Chapter 3 Reading in data locally and from the web 3.1 Overview 3.2 Learning Objectives", " Chapter 3 Reading in data locally and from the web 3.1 Overview Learn to read in various cases of tabular data sets locally and from the web. Once read in, these data sets will be used to walk through a real world Data Science application that includes wrangling the data into a useable format and creating an effective data visualization. 3.2 Learning Objectives By the end of the chapter, students will be able to: define the following: absolute file path relative file path url match the following tidyverse read_* function arguments to their descriptions: file delim col_names skip choose the appropriate tidyverse read_* function and function arguments to load a given tabular data set into R use the rvest html_nodes, html_text and html_attr functions to scrape data from a .html file on the web compare downloading tabular data from a plain text file (e.g., .csv) from the web versus scraping data from a .html file "],
["chapter4.html", "Chapter 4 Cleaning and wrangling data 4.1 Overview 4.2 Learning Objectives", " Chapter 4 Cleaning and wrangling data 4.1 Overview This week will be centered around tools for cleaning and wrangling data. Again, this will be in the context of a real world Data Science application and we will continue to practice working through a whole case study. 4.2 Learning Objectives By the end of the chapter, students will be able to: define the term “tidy data” discuss the advantages and disadvantages from storing data in a tidy data format recall and use the following tidyverse functions for their intended data wrangling tasks: select filter map mutate summarise group_by gather "],
["chapter5.html", "Chapter 5 Effective data visualization 5.1 Overview 5.2 Learning Objectives", " Chapter 5 Effective data visualization 5.1 Overview Expand your data visualization knowledge and tool set beyond what we have seen and practiced so far. We will move beyond scatter plots and learn other effective ways to visualize data, as well as some general rules of thumb to follow when creating visualations. All visualization tasks this week will be applied to real world data sets. 5.2 Learning Objectives By the end of the chapter, students will be able to: Define the three key aspects of ggplot objects: aesthetic mappings geometric objects scales Use the ggplot2 function in R to create the following visualizations: 2-D scatter plot 2-D scatter plot with a third variable that stratifies the groups count bar chart for multiple groups proportion bar chart for multiple groups stacked bar chart for multiple groups List the rules of thumb for effective visualizations Given a visualization and a sentence describing it’s intended task, evaluate it’s effectiveness and suggest ways to improve the visualization with respect to that intended task "],
["chapter6.html", "Chapter 6 Classification 6.1 Learning objectives 6.2 Classification 6.3 Wisconsin Breast Cancer Example:", " Chapter 6 Classification 6.1 Learning objectives Recognize situations where a simple classifier would be appropriate for making predictions. Explain the k-nearest neighbour classification algorithm. Interpret the output of a classifier. Compute, by hand, the distance between points when there are two attributes. Describe what a training data set is and how it is used in classification. In a dataset with two attributes, perform k-nearest neighbour classification in R using caret::train(method = “knn”, …) to predict the class of a single new observation. 6.2 Classification In many situations, we want to learn how to make predictions based on our experience from past examples. For instance, a doctor wants to diagnose a patient as either diseased or healthy based on some observed characteristics, an email provider would like to assign a given email as “spam” or “non-spam”, or an online store wants to predict if an order is fraudulent. These are all examples of classification tasks. Classification is the problem of predicting a qualitative or categorical response for an observation. It involves assigning an observation to a class (e.g. disease or healthy) on the basis of how similar they are to other observations that have already been classified. These already classified observations that we use as a basis to predict classes for new, unclassfied observations is called a training set. We call them a “training set” because we use these observations to train, or teach, our classifier so that we can make predictions on new data that we have not seen previously. There are many possible classifiers that we could use to predict a qualitative response. These classification methods can perform binary classification, only two classes are involved (e.g. disease or healthy patient), but we can also have multiclass classification, which involves assigning an object to one of several classes (e.g., private, public, or not for-profit organization). Here we will focus on a simple, and widely used method of classification called K-nearest neighbors, but others include decision trees, support vector machines and logistic regression. 6.3 Wisconsin Breast Cancer Example: Let’s start by looking at some Breast Cancer data, which was obtained from the University of Wisconsin Hospitals, Madison from Dr. William H. Wolberg. Each row in the data set represents an observation whose diagnosis class is known (benign/non-cancerous or malignant/cancerous) and for each, we have several attributes (texture, perimeter etc.). We’d like to find which attributes are most useful for diagnosing benign or malignant tumours, and develop a way to classify future patients. Benign tumours are not normally dangerous, the cells stay in the same place and the tumour stops growing before it gets very large. In malignant tumours, the cells invade the surrounding tissue and spread into nearby organs where they can cause serious damage. (https://www.worldwidecancerresearch.org/who-we-are/cancer-basics/) 6.3.1 Data Exploration suppressMessages({ library(tidyverse) library(ggplot2) library(forcats) # fct_recode() }) bcDat &lt;- read_csv(&quot;data/clean-wdbc.data.csv&quot;) ## Parsed with column specification: ## cols( ## ID = col_integer(), ## Class = col_character(), ## Radius = col_double(), ## Texture = col_double(), ## Perimeter = col_double(), ## Area = col_double(), ## Smoothness = col_double(), ## Compactness = col_double(), ## Concavity = col_double(), ## Concave_points = col_double(), ## Symmetry = col_double(), ## Fractal_dimension = col_double() ## ) 6.3.1.1 Attribute description Breast tumours can be diagnosed by performing a biopsy, a process where tissue is removed from the body to discover the presence of a disease. Fine needle asipiration is a type of biopsy that uses a thin needle to examine a small amount of tissue from the tumour. Attributes in this dataset are computed from an image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. Source: https://www.semanticscholar.org/paper/Breast-Cancer-Diagnosis-and-Prognosis-Via-Linear-P-Mangasarian-Street/3721bb14b16e866115c906336e9d70db096c05b9/figure/0 A magnified image of a malignant breast Fine Needle Aspiration ID number Class - diagnosis (M = malignant, B = benign) radius (mean of distances from center to points on the perimeter) texture (standard deviation of gray-scale values) perimeter area smoothness (local variation in radius lengths) compactness (perimeter^2 / area - 1.0) concavity (severity of concave portions of the contour) concave points (number of concave portions of the contour) symmetry fractal dimension (“coastline approximation” - 1) The “worst” or largest (mean of the three largest values) of these features were computed for each image. As part of the data preparation, the data have been scaled. glimpse(bcDat) ## Observations: 569 ## Variables: 12 ## $ ID &lt;int&gt; 842302, 842517, 84300903, 84348301, 84358402... ## $ Class &lt;chr&gt; &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;,... ## $ Radius &lt;dbl&gt; 1.8850310, 1.8043398, 1.5105411, -0.2812170,... ## $ Texture &lt;dbl&gt; -1.35809849, -0.36887865, -0.02395331, 0.133... ## $ Perimeter &lt;dbl&gt; 2.30157548, 1.53377643, 1.34629062, -0.24971... ## $ Area &lt;dbl&gt; 1.999478159, 1.888827020, 1.455004298, -0.54... ## $ Smoothness &lt;dbl&gt; 1.306536657, -0.375281748, 0.526943750, 3.39... ## $ Compactness &lt;dbl&gt; 2.61436466, -0.43006581, 1.08198014, 3.88997... ## $ Concavity &lt;dbl&gt; 2.10767182, -0.14661996, 0.85422232, 1.98783... ## $ Concave_points &lt;dbl&gt; 2.29405760, 1.08612862, 1.95328166, 2.173873... ## $ Symmetry &lt;dbl&gt; 2.7482041, -0.2436753, 1.1512420, 6.0407261,... ## $ Fractal_dimension &lt;dbl&gt; 1.93531174, 0.28094279, 0.20121416, 4.930671... We can see from the summary of the data above that “Class” is an character variable. We are going to be working with “Class” as a categorical variable so we will convert it to factor. bcDat &lt;- bcDat %&gt;% mutate(Class = as.factor(Class)) bcDat$Class %&gt;% levels() ## [1] &quot;B&quot; &quot;M&quot; In our dataset, we have 357 (63%) benign and 212 (37%) malignant observations. bcDat %&gt;% group_by(Class) %&gt;% tally() ## # A tibble: 2 x 2 ## Class n ## &lt;fct&gt; &lt;int&gt; ## 1 B 357 ## 2 M 212 Let’s draw a scatter plot to visualize the relationship between the perimeter and concavity variables. cbPalette &lt;- c(&quot;#999999&quot;, &quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;, &quot;#F0E442&quot;, &quot;#0072B2&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot;) # colour palette p &lt;- bcDat %&gt;% ggplot(aes(x=Perimeter, y=Concavity, color = Class)) + geom_point() + scale_x_continuous(name = &quot;Perimeter&quot;) + scale_y_continuous(name = &quot;Concavity&quot;) + scale_color_manual(values=c(cbPalette[3], cbPalette[2])) p Suppose we have a new observation that is not in the data set with perimeter 1 and concavity 1, would you classify that observation as benign or malignant? What about a new observation with perimeter -1 and concavity -0.5? What about 0 and 1? We can see that most of the benign observations are clustered in the lower-left corner and most of the malignant observations are above and to the right of that cluster, though not all the observations follow this pattern. We want to find a way to program a computer to automatically detect patterns. 6.3.2 K-Nearest Neighbour Classifier To classify a new observation as benign or malignant, we find some observations in the training set that are “nearest” to our new observation, and then use that diagnosis (benign or malignant) to make a prediction. Suppose we have a new observation, with perimeter of 2 and concavity of 4 (labelled in red on the scatterplot), whose “Class” is unknown. We see that the nearest point to this new observation is located at the coordinates (2.3, 3.2). The idea here is that if a point is close to one another in the scatterplot then the perimeter and concavity values are similar so we may expect that they would have the same diagnosis. Suppose we have another new observation with perimeter 0.38 and concavity of 1.8. Looking at the scatterplot below, how would you classify this red observation? The nearest neighbour to this new point is a benign observation at (0.2, 1.8). We can look at a few points, say \\(k = 3\\), that are closest to the new red observation to predict its class rather than just looking at one. Among those 3 closest points, we look at their class and use the majority class as our prediction for the new observation. We see that the diagnoses of 2 of the 3 nearest neighbours to our new observation are malignant so we take majority vote and classify our new red observation as malignant. ## # A tibble: 3 x 4 ## ID Perimeter Concavity Class ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 9113239 0.230 1.84 B ## 2 8511133 0.531 1.72 M ## 3 899667 0.361 1.99 M Here we chose the \\(k=3\\) nearest observations, but there is nothing special about \\(k=3\\). We could have used \\(k=4, 5\\) or more, though we may want to choose an odd number to avoid ties. We will discuss more about choosing \\(k\\) in the next section. 6.3.2.1 Distance Between Points When There are Two Attributes How do we decide which points are “nearest” to our new observation? We can compute the distance between any pair of points using the following formula: \\[Distance = \\sqrt{(x_a -x_b)^2 + (y_a - y_b)^2}\\] Suppose we want to classify a new observation with perimeter of -1 and concavity of 4.2. Let’s calculate the distances between our new point and each of the observations in the training set to find the \\(k=5\\) observations in the training data that are nearest to our new point. ID Perimeter Concavity Distance Class 859471 -1.24 4.7 \\(\\sqrt{-1 - (-1.24))^2 + (4.2 - 4.7)^2}=\\) 0.55 B 84501001 -0.29 3.99 \\(\\sqrt{(-1 - (-0.29))^2 + (4.2 - 3.99)^2} =\\) 0.74 M 8710441 -1.08 2.63 \\(\\sqrt{(-1 - (-1.08))^2 + (4.2 - 2.63)^2} =\\) 1.57 B 9013838 -0.46 2.72 \\(\\sqrt{(-1 - (-0.46))^2 + (4.2 - 2.72)^2} =\\) 1.57 M 925622 0.64 4.3 \\(\\sqrt{(-1 - 0.64)^2 + (4.2 - 4.3)^2} =\\) 1.64 M From the table, we see that 3 of the 5 nearest neighbours to our new observation are malignant so classify our new observation as malignant. 6.3.2.1.1 Summary: In order to classify a new observation using a k-nearest neighbor classifier, we have to do the follow steps: Step 1: Compute the distance between the new observation and each observation in our training set. Step 2: Sort the data table in ascending order according to the distances. Step 3: Choose the top \\(k\\) rows of the sorted table. Step 4: Classify the new observation based on majority vote. 6.3.3 K-Nearest Neighbours in R We will implement the k-nearest neighbour algorithm in R. Here we will make use of the caret (Classification And REgression Training) package in R, which contains a set of tools to help the process of making predictive models. We can use the function names(getModelInfo()) to see a full list of the algorithms caret has to offer. library(caret) Let’s suppose we have a new observation with perimeter 0 and concavity 0.5, but its diagnosis is unknown. Suppose we want to use its perimeter and concavity attributes to predict the class of this observation. Let’s pick out our 2 desired attributes and store it as a new dataset. trainingDat &lt;- bcDat %&gt;% select(&quot;Perimeter&quot;, &quot;Concavity&quot;) glimpse(trainingDat) ## Observations: 569 ## Variables: 2 ## $ Perimeter &lt;dbl&gt; 2.30157548, 1.53377643, 1.34629062, -0.24971958, 1.3... ## $ Concavity &lt;dbl&gt; 2.10767182, -0.14661996, 0.85422232, 1.98783917, 0.6... We will store the class labels in a vector. labels &lt;- bcDat$Class glimpse(labels) ## Factor w/ 2 levels &quot;B&quot;,&quot;M&quot;: 2 2 2 2 2 2 2 2 2 2 ... We will use the function train(), where x is an object where observations are in rows and the attributes are in columns (e.g. simple matrix, dataframe) and y is a numeric or factor vector containing the outcome for each row. The argument tuneGrid should be a dataframe with possible “tuning values”. For now, just know that this is where we will specify our \\(k\\) and we will use \\(k =7\\) (we will discuss how to choose \\(k\\) in a later section). We will use “knn” as our method. k &lt;- data.frame(k = 7) model_knn &lt;- train(x = data.frame(trainingDat), y = labels, method=&#39;knn&#39;, tuneGrid = k) Now we can predict the label of the new observation. new_obs &lt;- data.frame(Perimeter = 0, Concavity = 0.5) predict(object=model_knn, new_obs) ## [1] M ## Levels: B M Our model classifies this new observation as malignant. How do we know how well our model did? In later sections, we will discuss ways to evaluate our model. 6.3.4 Multiple Attributes So far we have seen how to build a classifier based on only two attributes, but we can use k-nearest neighbours classifier in higher dimensional space. Let’s make a scatterplot with 3 variables instead of 2: Each attribute can give us new information to help create our classifier. The distance formula for 3-dimensions is \\[Distance = \\sqrt{(x_a -x_b)^2 + (y_a - y_b)^2 + (z_a - z_b)^2}\\] We can generalize for n-dimensions by summing up the squares of the differences between each individual coordinate taking the square root of the sum Data source: W.N. Street, W.H. Wolberg and O.L. Mangasarian Nuclear feature extraction for breast tumor diagnosis. IS&amp;T/SPIE 1993 International Symposium on Electronic Imaging: Science and Technology, volume 1905, pages 861-870, San Jose, CA, 1993. "],
["chapter7.html", "Chapter 7 Classification, continued 7.1 Overview 7.2 Learning Objectives 7.3 Data set", " Chapter 7 Classification, continued 7.1 Overview Metrics for classification accuracy; cross-validation to choose the number of neighbours; scaling of variables and other practical considerations. 7.2 Learning Objectives Describe what a test data set is and how it is used in classification. Using R, evaluate classification accuracy using a test data set and appropriate metrics. Using R, execute cross validation in R to choose the number of neighbours. Identify when it is necessary to scale variables before classification and do this using R In a dataset with &gt; 2 attributes, perform k-nearest neighbour classification in R using caret::train(method = “knn”, …) to predict the class of a test dataset. Describe advantages and disadvantages of the k-nearest neighbour classification algorithm. 7.3 Data set Keep working with the Wisconsin Breast Cancer data set. "],
["chapter8.html", "Chapter 8 Clustering 8.1 Learning Objectives", " Chapter 8 Clustering 8.1 Learning Objectives By the end of the chapter, students will be able to: Describe a case where clustering would be an appropriate tool, and what insight it would bring from the data. Explain the kmeans clustering algorithm. Interpret the output of a kmeans cluster analysis. Perform kmeans clustering in R using kmeans Visualize the output of kmeans clustering in R using pair-wise scatter plots Identify when it is necessary to scale variables before clustering and do this using R Use the elbow method to choose the number of clusters for k-means Describe advantages, limitations and assumptions of the kmeans clustering algorithm. "],
["chapter9.html", "Chapter 9 Regression 9.1 Overview 9.2 Learning Objectives 9.3 Dataset Ideas", " Chapter 9 Regression 9.1 Overview This week will introduce the problem of finding and estimating/approximating the relationship between a response variable of interest and a group of potential explanatory variables. The focus will be on prediction, relying on graphical tools to help the students “discover” simple non-parametric regression estimators (e.g. nearest neighbours, maybe kernel regression and/or regression trees). The prediction properties of these estimators will be explored using cross-validation. 9.2 Learning Objectives Placeholder 9.3 Dataset Ideas UCI Regression Datasets Conditional Release (Decisions) by Parole Board of Canada. Students could investigate factors affecting parole hearing decisions. Data from one year (FY 2015-2016) contains ~22,000 rows and 19 columns. Factors include: Final Decision Decision Month Review Type Decision Type Sentence Type Jurisdiction Gender Race Major Offence Group "],
["chapter10.html", "Chapter 10 Regression, continued 10.1 Overview 10.2 Learning Objectives", " Chapter 10 Regression, continued 10.1 Overview This week will focus on the distinction between goodness of fit and prediction properties (namely MSE vs MSPE). Again, based on an example (but this time higher-dimensional) we will compare the predictions obtained from highly adaptive but variable/unstable methods (e.g. nearest neighbours, large regression trees) with those from more restricted but stable ones (e.g. linear models). The interpretative advantage of linear models will be discussed as well. 10.2 Learning Objectives Placeholder "],
["chapter11.html", "Chapter 11 Regression, continued some more… 11.1 Overview 11.2 Learning Objectives", " Chapter 11 Regression, continued some more… 11.1 Overview This week will introduce the bootstrap, first by visualizing bootstrap samples and their fitted regression lines for a bivariate dataset. An intuitive case will be made for what the ensemble of slopes represents, Then we work through examples from multiple regression, emphasizing the scientific interpretation and relevance of the mix of negative/positive slopes. We will emphasize that this is a jumping off point for the study of statistical inference, as per STAT 200 or a similar course. 11.2 Learning Objectives Placeholder "],
["references.html", "References", " References "]
]
